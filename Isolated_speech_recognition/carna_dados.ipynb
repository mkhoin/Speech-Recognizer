{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import all modules\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import mir3.modules.features as feat\n",
    "import mir3.modules.tool.wav2spectrogram as spec\n",
    "import mir3.modules.features.centroid as cent\n",
    "import mir3.modules.features.rolloff as roll\n",
    "import mir3.modules.features.flatness as flat\n",
    "import mir3.modules.features.flux as flux\n",
    "import mir3.modules.features.mfcc as mfcc\n",
    "import mir3.modules.features.diff as diff\n",
    "import mir3.modules.features.stats as stats\n",
    "reload(stats)\n",
    "import mir3.modules.features.join as join\n",
    "import mir3.modules.tool.to_texture_window as tex\n",
    "\n",
    "import copy\n",
    "\n",
    "from scipy.stats import friedmanchisquare as friedman\n",
    "from scipy.stats import wilcoxon as wilcoxon\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "from scipy.stats import f_oneway as f_oneway\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import hmmlearn\n",
    "from hmmlearn.hmm import GaussianHMM, MultinomialHMM\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from scipy.stats import friedmanchisquare as friedman\n",
    "from scipy.stats import wilcoxon as wilcoxon\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "from scipy.stats import f_oneway as f_oneway\n",
    "\n",
    "f = open('Resultados_F_SemCV.txt', 'w') #save in txt\n",
    "\n",
    "#Run all train dataset\n",
    "\n",
    "#bases_treino = [\"./fernando_falas/\"]#, \"./dataset_noise_h/noise_10/\", \"./dataset_noise_h/noise_20/\", \"./dataset_noise_h/noise_30/\", \"./dataset_noise_h/noise_40/\", \"./dataset_noise_h/noise_50/\",\"./dataset_noise_h/noise_60/\", \"./dataset_noise_h/noise_70/\", \"./dataset_noise_h/noise_80/\", \"./dataset_noise_h/noise_90/\", \"./dataset_noise_h/noise_100/\"]\n",
    "#rotulos_treino = [\"labels.txt\",\"label_noise_10.txt\",\"label_noise_20.txt\", \"label_noise_30.txt\", \"label_noise_40.txt\", \"label_noise_50.txt\", \"label_noise_60.txt\", \"label_noise_70.txt\", \"label_noise_80.txt\", \"label_noise_90.txt\", \"label_noise_100.txt\"]\n",
    "\n",
    "bases_treino = [\"./fernando_falas/\", \"./dataset_noise_f/noise_10/\", \"./dataset_noise_f/noise_20/\", \"./dataset_noise_f/noise_30/\", \"./dataset_noise_f/noise_40/\", \"./dataset_noise_f/noise_50/\",\"./dataset_noise_f/noise_60/\", \"./dataset_noise_f/noise_70/\", \"./dataset_noise_f/noise_80/\", \"./dataset_noise_f/noise_90/\", \"./dataset_noise_f/noise_100/\"]\n",
    "rotulos_treino = [\"labels.txt\",\"label_noise_10.txt\",\"label_noise_20.txt\", \"label_noise_30.txt\", \"label_noise_40.txt\", \"label_noise_50.txt\", \"label_noise_60.txt\", \"label_noise_70.txt\", \"label_noise_80.txt\", \"label_noise_90.txt\", \"label_noise_100.txt\"]\n",
    " \n",
    "\n",
    "for a in xrange(len(bases_treino)):\n",
    "    \n",
    "    def dataset_characterization(dataset_file, dataset_dir=\"\"):\n",
    "        dataset = {} # Dictionary index = filename, content = class\n",
    "        with open(dataset_dir + dataset_file) as f:\n",
    "            for line in f:\n",
    "                p = re.split(\" |,|\\t\", line.rstrip('\\n').rstrip('\\r'))\n",
    "                dataset[p[0]] = p[1]\n",
    "        return dataset\n",
    "\n",
    "    def dataset_class_histogram(dataset):\n",
    "        histogram = {}\n",
    "        for data in dataset:\n",
    "            if dataset[data] not in histogram:\n",
    "                histogram[dataset[data]] = 1\n",
    "            else:\n",
    "                histogram[dataset[data]] += 1\n",
    "        return histogram\n",
    "\n",
    "    dataset_dir = bases_treino[a]\n",
    "    dataset_file = rotulos_treino[a]\n",
    "\n",
    "    #dataset_dir = \"./datasets/gtzan/\"\n",
    "    #dataset_file = \"labels.txt\"\n",
    "    dataset = dataset_characterization(dataset_file, dataset_dir)\n",
    "    print \"Base de treino, \", bases_treino[a], \": \", dataset_class_histogram(dataset)\n",
    "\n",
    "\n",
    "\n",
    "    def features_gtzan(filename, directory=\"\"):\n",
    "        # Calculate spectrogram (normalizes wavfile)\n",
    "        converter = spec.Wav2Spectrogram()\n",
    "        s = converter.convert(open(directory + filename), window_length=512, dft_length=512,\n",
    "                    window_step=256, spectrum_type='magnitude', save_metadata=True, wav_rate=16000)\n",
    "\n",
    "        # Extract low-level features, derivatives, and run texture windows    \n",
    "\n",
    "        d = diff.Diff()\n",
    "        #features = (cent.Centroid(), roll.Rolloff(), flat.Flatness(), flux.Flux(), mfcc.Mfcc())\n",
    "        features = (mfcc.Mfcc(),)\n",
    "        all_feats = None\n",
    "        for f in features:\n",
    "            track = f.calc_track(s) # Feature track\n",
    "            all_feats = join.Join().join([all_feats, track])\n",
    "            dtrack = d.calc_track(track) # Differentiate\n",
    "            all_feats = join.Join().join([all_feats, dtrack])\n",
    "            ddtrack = d.calc_track(dtrack) # Differentiate again\n",
    "            all_feats = join.Join().join([all_feats, ddtrack])    \n",
    "\n",
    "            # Texture window\n",
    "        #t = tex.ToTextureWindow().to_texture(all_feats, 40)\n",
    "\n",
    "        # Statistics\n",
    "        #s = stats.Stats()\n",
    "        #d = s.stats([t], mean=True, variance=True)    \n",
    "        return all_feats\n",
    "    from ipywidgets import FloatProgress\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    def low_level_features(dataset_file, dataset_dir=\"\"): # Estimate low-level features. \n",
    "                                                          # Returns sklearn-compatible structures.\n",
    "        d = dataset_characterization(dataset_file, dataset_dir)\n",
    "        labels = []\n",
    "        features = []\n",
    "        progress = FloatProgress(min=0, max=len(d.keys()))\n",
    "        display(progress)\n",
    "        progress.value = 0\n",
    "        for f in d:\n",
    "            feat = features_gtzan(filename=f, directory=dataset_dir)\n",
    "            if not np.any(np.isnan(feat.data)):\n",
    "                features.append(feat.data)\n",
    "                labels.append(d[f])\n",
    "            progress.value += 1\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset_characterization(dataset_file, dataset_dir)\n",
    "    features, labels = low_level_features(dataset_file, dataset_dir)\n",
    "    \n",
    "    #bases_teste = [\"./fernando_falas/\", \"./dataset_noise_h/noise_10/\", \"./dataset_noise_h/noise_20/\", \"./dataset_noise_h/noise_30/\", \"./dataset_noise_h/noise_40/\", \"./dataset_noise_h/noise_50/\",\"./dataset_noise_h/noise_60/\", \"./dataset_noise_h/noise_70/\", \"./dataset_noise_h/noise_80/\", \"./dataset_noise_h/noise_90/\", \"./dataset_noise_h/noise_100/\"]\n",
    "    #rotulos_teste = [\"labels.txt\",\"label_noise_10.txt\",\"label_noise_20.txt\", \"label_noise_30.txt\", \"label_noise_40.txt\", \"label_noise_50.txt\", \"label_noise_60.txt\", \"label_noise_70.txt\", \"label_noise_80.txt\", \"label_noise_90.txt\", \"label_noise_100.txt\"]\n",
    "    \n",
    "    bases_teste = [\"./fernando_falas/\", \"./dataset_noise_f/noise_10/\", \"./dataset_noise_f/noise_20/\", \"./dataset_noise_f/noise_30/\", \"./dataset_noise_f/noise_40/\", \"./dataset_noise_f/noise_50/\",\"./dataset_noise_f/noise_60/\", \"./dataset_noise_f/noise_70/\", \"./dataset_noise_f/noise_80/\", \"./dataset_noise_f/noise_90/\", \"./dataset_noise_f/noise_100/\"]\n",
    "    rotulos_teste = [\"labels.txt\",\"label_noise_10.txt\",\"label_noise_20.txt\", \"label_noise_30.txt\", \"label_noise_40.txt\", \"label_noise_50.txt\", \"label_noise_60.txt\", \"label_noise_70.txt\", \"label_noise_80.txt\", \"label_noise_90.txt\", \"label_noise_100.txt\"]\n",
    " \n",
    "       \n",
    "    #run all test dataset\n",
    "    for rev in xrange(len(bases_teste)):\n",
    "\n",
    "        #dowloads datasets\n",
    "        def dataset_characterization(dataset_file2, dataset_dir2=\"\"):\n",
    "            dataset2 = {} # Dictionary index = filename, content = class\n",
    "            with open(dataset_dir2 + dataset_file2) as f2:\n",
    "                for line in f2:\n",
    "                    p = re.split(\" |,|\\t\", line.rstrip('\\n').rstrip('\\r'))\n",
    "                    dataset2[p[0]] = p[1]\n",
    "            return dataset2\n",
    "\n",
    "        def dataset_class_histogram(dataset2):\n",
    "            histogram = {}\n",
    "            for data2 in dataset2:\n",
    "                if dataset2[data2] not in histogram:\n",
    "                    histogram[dataset2[data2]] = 1\n",
    "                else:\n",
    "                    histogram[dataset2[data2]] += 1\n",
    "            return histogram\n",
    "        \n",
    "        dataset_dir2 = bases_teste[rev]\n",
    "        dataset_file2 = rotulos_teste[rev]\n",
    "\n",
    "        dataset2 = dataset_characterization(dataset_file2, dataset_dir2)\n",
    "        print \"base de teste, \", bases_teste[rev], \": \", dataset_class_histogram(dataset2)\n",
    "        \n",
    "        #Feature extraction\n",
    "        def low_level_features(dataset_file2, dataset_dir2=\"\"):\n",
    "            d = dataset_characterization(dataset_file2, dataset_dir2)\n",
    "            labels2 = []\n",
    "            features2 = []\n",
    "            progress = FloatProgress(min=0, max=len(d.keys()))\n",
    "            display(progress)\n",
    "            progress.value = 0\n",
    "            for f in d:\n",
    "                feat = features_gtzan(filename=f, directory=dataset_dir2)\n",
    "                if not np.any(np.isnan(feat.data)):\n",
    "                    features2.append(feat.data)\n",
    "                    labels2.append(d[f])\n",
    "                progress.value += 1\n",
    "            return features2, labels2\n",
    "\n",
    "        dataset2 = dataset_characterization(dataset_file2, dataset_dir2)\n",
    "        features2, labels2 = low_level_features(dataset_file2, dataset_dir2)\n",
    "        \n",
    "            \n",
    "        \n",
    "        #Classification\n",
    "        def label_to_numbers(labels, d):\n",
    "            return [d.keys().index(i) for i in labels]\n",
    "\n",
    "\n",
    "        def numbers_to_labels(numbers, d):\n",
    "            return [d.keys[i] for i in numbers]\n",
    "\n",
    "        def f1_from_confusion(c):\n",
    "            ret = []\n",
    "            for i in xrange(c.shape[0]):\n",
    "                n_i = np.sum(c[i,:])\n",
    "                est_i = np.sum(c[:,i])\n",
    "                if n_i > 0:\n",
    "                    R = c[i,i] / float(n_i)\n",
    "                else:\n",
    "                    R = 0.0\n",
    "                if est_i > 0:\n",
    "                    P = c[i,i] / float(est_i)\n",
    "                else:\n",
    "                    P = 0.0\n",
    "\n",
    "                if (R+P) > 0:\n",
    "                    F = 2*R*P/(R+P)\n",
    "                else:\n",
    "                    F = 0.\n",
    "                ret.append([R, P, F])\n",
    "            return ret\n",
    "\n",
    "        class MultipleHMM():\n",
    "            base_model = None\n",
    "            models = {}\n",
    "\n",
    "            def __init__(self, base_model=None):\n",
    "                self.base_model = base_model\n",
    "\n",
    "            def fit(self, X, y, train_lengths):\n",
    "                \"\"\"Fits all internal models\"\"\"\n",
    "                labels = set(y)\n",
    "                for l in labels:\n",
    "                    print \"Training label:\", l\n",
    "                    l_index = [i for i in xrange(len(y)) if y[i] == l]\n",
    "                    if X[0].ndim == 1:\n",
    "                        this_x = [X[i].reshape(-1, 1) for i in l_index]\n",
    "                    else:\n",
    "                        this_x = [X[i] for i in l_index]\n",
    "\n",
    "                    this_lengths = [train_lengths[i] for i in l_index]\n",
    "\n",
    "                    my_x = this_x[0]\n",
    "                    for i in xrange(1, len(this_x)):\n",
    "                        my_x = np.vstack ((my_x, np.array(this_x[i])))\n",
    "\n",
    "                    new_model = copy.deepcopy(self.base_model)\n",
    "                    new_model.fit(my_x, this_lengths)\n",
    "                    self.models[l] = new_model\n",
    "\n",
    "            def predict(self, X):\n",
    "                \"\"\"Predicts a label for input X\"\"\"\n",
    "                return_label = None\n",
    "                best_prob = None\n",
    "                for label in self.models.keys():\n",
    "\n",
    "                    #print \"Computing probabilities on shape\", X.shape\n",
    "                    if X.ndim == 1:\n",
    "                        this_prob = self.models[label].score(X.reshape(-1,1))\n",
    "                    else:\n",
    "                        this_prob = self.models[label].score(X)\n",
    "\n",
    "                    #print \"Prob in label\", label, \"=\", this_prob\n",
    "                    if (best_prob is None) or (this_prob > best_prob):\n",
    "                        best_prob = this_prob\n",
    "                        return_label = label\n",
    "                return return_label\n",
    "        \n",
    "        \n",
    "         #Statistical analysis\n",
    "        def model_comparison(features, labels, features2, labels2, models):\n",
    "            #norm_features = normalize(features)\n",
    "            features = np.array(features)\n",
    "            features2 = np.array(features2)\n",
    "            #skf = StratifiedKFold(labels)\n",
    "\n",
    "           # f1 = np.zeros((n_folds,len(models)))\n",
    "           # r = np.zeros((n_folds,len(models)))\n",
    "           # p = np.zeros((n_folds,len(models)))\n",
    "            #progress = FloatProgress(min=0, max=n_folds*len(models))\n",
    "            #display(progress)\n",
    "            for m in xrange(len(models)):\n",
    "                n = 0\n",
    "                #for train_index, test_index in skf:\n",
    "                X_train, X_test = features, features2\n",
    "                Y_train, Y_test = labels, labels2\n",
    "\n",
    "                all_trains = np.array(X_train[0])\n",
    "                train_lengths = [X_train[0].shape[0]]\n",
    "                for i in xrange(1, len(X_train)):\n",
    "                    all_trains = np.vstack ((all_trains, np.array(X_train[i])))\n",
    "                    train_lengths.append(X_train[i].shape[0])\n",
    "                all_trains = all_trains\n",
    "                #print train_lengths\n",
    "        \n",
    "                scaler = preprocessing.StandardScaler().fit(all_trains)\n",
    "                X_train = [scaler.transform(X_train[i]) for i in xrange(len(X_train))]\n",
    "                X_test = [scaler.transform(X_test[i]) for i in xrange(len(X_test))]\n",
    "\n",
    "                #cv = ShuffleSplit(len(X_train).shape[0], n_iter=10, test_size=0.2, random_state=0) # 80% train / 20% validation\n",
    "                #classifier = GridSearchCV(estimator=copy.deepcopy(models[m]), cv=cv, param_grid=parameters_to_optimize[m], scoring='f1_weighted')\n",
    "                classifier = models[m]\n",
    "\n",
    "                classifier.fit(X_train, Y_train, train_lengths)\n",
    "                Y_pred = [classifier.predict(X_test[i]) for i in xrange(len(X_test))]\n",
    "                #print Y_pred\n",
    "                confusion = confusion_matrix(Y_test, Y_pred)\n",
    "                print \"confusion matrix:\\n\", confusion\n",
    "                conf = f1_from_confusion(confusion)\n",
    "                conf_all = np.average(conf, axis=0)\n",
    "                print conf_all\n",
    "\n",
    "                r = conf_all[0]\n",
    "                p = conf_all[1]\n",
    "                f1 = conf_all[2]\n",
    "                n += 1\n",
    "\n",
    "            #progress.value += 1\n",
    "            return r, p, f1\n",
    "        \n",
    "        \n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        models = [MultipleHMM(GaussianHMM(n_components=12, covariance_type='diag', n_iter=50))]\n",
    "\n",
    "        dataset = dataset_characterization(dataset_file, dataset_dir)\n",
    "\n",
    "        recall, precision, f1_score = model_comparison(features, label_to_numbers(labels, dataset_class_histogram(dataset)), features2, label_to_numbers(labels2, dataset_class_histogram(dataset2)), models)\n",
    "        #print recall, precision, f1_score\n",
    "        print \"Train: \", bases_treino[a], \"Test: \", bases_teste[rev], \"\\nrecal:\",recall, \"\\nprecision:\", precision, \"\\nf1_score:\",f1_score, \"\\n\"\n",
    "        \n",
    "        f.write(\"Recall: {}\".format(recall))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Precision: {}\".format(precision))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"F1_score{}\".format(f1_score))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Train: {}\".format(bases_treino[a]))\n",
    "        f.write(\"Test: {}\".format(bases_teste[rev]))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "        if len(models) > 2:\n",
    "        #    print [f1_score[:,i].T for i in range(len(models))]\n",
    "            print \"Anova: \", f_oneway( *f1_score.T  )[1]\n",
    "\n",
    "        elif len(models) == 2:\n",
    "            print \"T-test:\", ttest( f1_score[:,0].T,  f1_score[:,1].T)[1]\n",
    "            \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
