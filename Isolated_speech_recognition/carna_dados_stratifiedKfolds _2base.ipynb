{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import all modules\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import mir3.modules.features as feat\n",
    "import mir3.modules.tool.wav2spectrogram as spec\n",
    "import mir3.modules.features.centroid as cent\n",
    "import mir3.modules.features.rolloff as roll\n",
    "import mir3.modules.features.flatness as flat\n",
    "import mir3.modules.features.flux as flux\n",
    "import mir3.modules.features.mfcc as mfcc\n",
    "import mir3.modules.features.diff as diff\n",
    "import mir3.modules.features.stats as stats\n",
    "reload(stats)\n",
    "import mir3.modules.features.join as join\n",
    "import mir3.modules.tool.to_texture_window as tex\n",
    "\n",
    "import copy\n",
    "\n",
    "from scipy.stats import friedmanchisquare as friedman\n",
    "from scipy.stats import wilcoxon as wilcoxon\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "from scipy.stats import f_oneway as f_oneway\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "import hmmlearn\n",
    "from hmmlearn.hmm import GaussianHMM, MultinomialHMM\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from scipy.stats import friedmanchisquare as friedman\n",
    "from scipy.stats import wilcoxon as wilcoxon\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "from scipy.stats import f_oneway as f_oneway\n",
    "import operator\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def dataset_characterization(dataset_file, dataset_dir=\"\"):\n",
    "    dataset = {} # Dictionary index = filename, content = class\n",
    "    with open(dataset_dir + dataset_file) as f:\n",
    "        for line in f:\n",
    "            p = re.split(\" |,|\\t\", line.rstrip('\\n').rstrip('\\r'))\n",
    "            #p = sorted(p)\n",
    "            dataset[p[0]] = p[1]\n",
    "            \n",
    "    return sorted(dataset.items(), key=operator.itemgetter(0))\n",
    "\n",
    "def dataset_class_histogram(dataset):\n",
    "    histogram = {}\n",
    "    for data in dataset:\n",
    "        #print data[1]\n",
    "        #print file_name\n",
    "        if data[1] not in histogram:\n",
    "            histogram[data[1]] = 1\n",
    "        else:\n",
    "            histogram[data[1]] += 1\n",
    "    return histogram\n",
    "\n",
    "dataset_dir = \"./fernando_falas/\"\n",
    "dataset_file = \"labels.txt\"\n",
    "\n",
    "dataset = dataset_characterization(dataset_file, dataset_dir)\n",
    "print dataset_class_histogram(dataset)\n",
    "\n",
    "dataset_dir2 = \"./dataset_noise_f/noise_70/\"\n",
    "dataset_file2 = \"label_noise_70.txt\"\n",
    "\n",
    "dataset2 = dataset_characterization(dataset_file2, dataset_dir2)\n",
    "print dataset_class_histogram(dataset2)\n",
    "\n",
    "def features_gtzan(filename, directory=\"\"):\n",
    "    # Calculate spectrogram (normalizes wavfile)\n",
    "    converter = spec.Wav2Spectrogram()\n",
    "    s = converter.convert(open(directory + filename), window_length=512, dft_length=512,\n",
    "                window_step=256, spectrum_type='magnitude', save_metadata=True, wav_rate=16000)\n",
    "    \n",
    "    # Extract low-level features, derivatives, and run texture windows    \n",
    "    \n",
    "    d = diff.Diff()\n",
    "    #features = (cent.Centroid(), roll.Rolloff(), flat.Flatness(), flux.Flux(), mfcc.Mfcc())\n",
    "    features = (mfcc.Mfcc(),)\n",
    "    all_feats = None\n",
    "    for f in features:\n",
    "        track = f.calc_track(s) # Feature track\n",
    "        all_feats = join.Join().join([all_feats, track])\n",
    "        dtrack = d.calc_track(track) # Differentiate\n",
    "        all_feats = join.Join().join([all_feats, dtrack])\n",
    "        ddtrack = d.calc_track(dtrack) # Differentiate again\n",
    "        all_feats = join.Join().join([all_feats, ddtrack])    \n",
    "\n",
    "        # Texture window\n",
    "    #t = tex.ToTextureWindow().to_texture(all_feats, 40)\n",
    "        \n",
    "    # Statistics\n",
    "    #s = stats.Stats()\n",
    "    #d = s.stats([t], mean=True, variance=True)    \n",
    "    return all_feats\n",
    "\n",
    "def low_level_features(dataset_file, dataset_dir=\"\"): # Estimate low-level features. \n",
    "                                                      # Returns sklearn-compatible structures.\n",
    "    d = dataset_characterization(dataset_file, dataset_dir)\n",
    "    #print d\n",
    "    labels = []\n",
    "    features = []\n",
    "    progress = FloatProgress(min=0, max=len(d))\n",
    "    display(progress)\n",
    "    progress.value = 0\n",
    "    for f in d:\n",
    "        feat = features_gtzan(filename=f[0], directory=dataset_dir)\n",
    "        if not np.any(np.isnan(feat.data)):\n",
    "            features.append(feat.data)\n",
    "            labels.append(f[1])\n",
    "        progress.value += 1\n",
    "    return features, labels\n",
    "\n",
    "dataset = dataset_characterization(dataset_file, dataset_dir)\n",
    "features, labels = low_level_features(dataset_file, dataset_dir)\n",
    "\n",
    "dataset2 = dataset_characterization(dataset_file2, dataset_dir2)\n",
    "features2, labels2 = low_level_features(dataset_file2, dataset_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bases_teste = [\"./fernando_falas/\", \"./dataset_noise_f/noise_10/\", \"./dataset_noise_f/noise_20/\", \"./dataset_noise_f/noise_30/\", \"./dataset_noise_f/noise_40/\", \"./dataset_noise_f/noise_50/\",\"./dataset_noise_f/noise_60/\", \"./dataset_noise_f/noise_70/\", \"./dataset_noise_f/noise_80/\", \"./dataset_noise_f/noise_90/\", \"./dataset_noise_f/noise_100/\"]\n",
    "rotulos_teste = [\"labels.txt\",\"label_noise_10.txt\",\"label_noise_20.txt\", \"label_noise_30.txt\", \"label_noise_40.txt\", \"label_noise_50.txt\", \"label_noise_60.txt\", \"label_noise_70.txt\", \"label_noise_80.txt\", \"label_noise_90.txt\", \"label_noise_100.txt\"]\n",
    " \n",
    "    \n",
    "#bases_teste = [\"./dataset_noise_f/noise_80/\", \"./dataset_noise_f/noise_90/\", \"./dataset_noise_f/noise_100/\"] \n",
    "#rotulos_teste = [\"label_noise_80.txt\", \"label_noise_90.txt\", \"label_noise_100.txt\"]\n",
    "\n",
    "       \n",
    "    #Rodar todas as bases de teste\n",
    "for rev in xrange(len(bases_teste)):\n",
    "\n",
    "    #dowloads datasets\n",
    "    \n",
    "    dataset_dir3 = bases_teste[rev]\n",
    "    dataset_file3 = rotulos_teste[rev]\n",
    "\n",
    "    dataset3 = dataset_characterization(dataset_file3, dataset_dir3)\n",
    "    print \"base de teste, \", bases_teste[rev], \": \", dataset_class_histogram(dataset3)\n",
    "        \n",
    "    #Feature extraction\n",
    "    dataset3 = dataset_characterization(dataset_file3, dataset_dir3)\n",
    "    features3, labels3 = low_level_features(dataset_file3, dataset_dir3)\n",
    "             \n",
    "        \n",
    "    #Classification\n",
    "    def label_to_numbers(labels, d):\n",
    "        return [d.keys().index(i) for i in labels]\n",
    "\n",
    "\n",
    "    def numbers_to_labels(numbers, d):\n",
    "        return [d.keys[i] for i in numbers]\n",
    "\n",
    "    def f1_from_confusion(c):\n",
    "        ret = []\n",
    "        for i in xrange(c.shape[0]):\n",
    "            n_i = np.sum(c[i,:])\n",
    "            est_i = np.sum(c[:,i])\n",
    "            if n_i > 0:\n",
    "                R = c[i,i] / float(n_i)\n",
    "            else:\n",
    "                R = 0.0\n",
    "            if est_i > 0:\n",
    "                P = c[i,i] / float(est_i)\n",
    "            else:\n",
    "                P = 0.0\n",
    "\n",
    "            if (R+P) > 0:\n",
    "                F = 2*R*P/(R+P)\n",
    "            else:\n",
    "                F = 0.\n",
    "            ret.append([R, P, F])\n",
    "        return ret\n",
    "\n",
    "    class MultipleHMM():\n",
    "        base_model = None\n",
    "        models = {}\n",
    "\n",
    "        def __init__(self, base_model=None):\n",
    "            self.base_model = base_model\n",
    "\n",
    "        def fit(self, X, y, train_lengths):\n",
    "            \"\"\"Fits all internal models\"\"\"\n",
    "            labels = set(y)\n",
    "            for l in labels:\n",
    "                print \"Training label:\", l\n",
    "                l_index = [i for i in xrange(len(y)) if y[i] == l]\n",
    "                if X[0].ndim == 1:\n",
    "                    this_x = [X[i].reshape(-1, 1) for i in l_index]\n",
    "                else:\n",
    "                    this_x = [X[i] for i in l_index]\n",
    "\n",
    "                this_lengths = [train_lengths[i] for i in l_index]\n",
    "\n",
    "                my_x = this_x[0]\n",
    "                for i in xrange(1, len(this_x)):\n",
    "                    my_x = np.vstack ((my_x, np.array(this_x[i])))\n",
    "\n",
    "                new_model = copy.deepcopy(self.base_model)\n",
    "                new_model.fit(my_x, this_lengths)\n",
    "                self.models[l] = new_model\n",
    "\n",
    "        def predict(self, X):\n",
    "            \"\"\"Predicts a label for input X\"\"\"\n",
    "            return_label = None\n",
    "            best_prob = None\n",
    "            for label in self.models.keys():\n",
    "\n",
    "                #print \"Computing probabilities on shape\", X.shape\n",
    "                if X.ndim == 1:\n",
    "                    this_prob = self.models[label].score(X.reshape(-1,1))\n",
    "                else:\n",
    "                    this_prob = self.models[label].score(X)\n",
    "\n",
    "                #print \"Prob in label\", label, \"=\", this_prob\n",
    "                if (best_prob is None) or (this_prob > best_prob):\n",
    "                    best_prob = this_prob\n",
    "                    return_label = label\n",
    "            return return_label\n",
    "        \n",
    "        \n",
    "    #Statistical analysis\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    n_folds = 10\n",
    "        \n",
    "    def model_comparison(features, labels, features2, labels2, features3, labels3, models): \n",
    "        #norm_features = normalize(features)\n",
    "        features = np.array(features)\n",
    "        features2 = np.array(features2)\n",
    "        features3 = np.array(features3)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=n_folds)\n",
    "\n",
    "        f1 = np.zeros((n_folds,len(models)))\n",
    "        r = np.zeros((n_folds,len(models)))\n",
    "        p = np.zeros((n_folds,len(models)))\n",
    "\n",
    "        progress = FloatProgress(min=0, max=n_folds*len(models))\n",
    "        display(progress)\n",
    "        for m in xrange(len(models)):\n",
    "            n = 0\n",
    "            for train_index, test_index in skf.split(features, labels):\n",
    "                #print( \"TRAIN:\", train_index, \"\\nTEST:\", test_index)\n",
    "                X_train1, X_train2, X_test = [features[i] for i in train_index], [features2[i] for i in train_index], [features3[i] for i in test_index]\n",
    "                Y_train1, Y_train2, Y_test = [labels[i] for i in train_index], [labels2[i] for i in train_index], [labels3[i] for i in test_index]\n",
    "\n",
    "                #X_train = np.vstack((X_train1, X_train2))\n",
    "                X_train = np.concatenate((X_train1,X_train2), axis=0)\n",
    "                #X_train = X_train1 + X_train2\n",
    "                Y_train = Y_train1 + Y_train2\n",
    "                \n",
    "               \n",
    "\n",
    "                all_trains = np.array(X_train[0])\n",
    "                train_lengths = [X_train[0].shape[0]]\n",
    "                for i in xrange(1, len(X_train)):\n",
    "                    all_trains = np.vstack ((all_trains, np.array(X_train[i])))\n",
    "                    train_lengths.append(X_train[i].shape[0])\n",
    "                all_trains = all_trains    \n",
    "                  \n",
    "                scaler = preprocessing.StandardScaler().fit(all_trains)\n",
    "                X_train = [scaler.transform(X_train[i]) for i in xrange(len(X_train))]\n",
    "                #X_train2 = [scaler.transform(X_train2[i]) for i in xrange(len(X_train2))]\n",
    "                X_test = [scaler.transform(X_test[i]) for i in xrange(len(X_test))]\n",
    "\n",
    "\n",
    "                #cv = ShuffleSplit(len(X_train).shape[0], n_iter=10, test_size=0.2, random_state=0) # 80% train / 20% validation\n",
    "                #classifier = GridSearchCV(estimator=copy.deepcopy(models[m]), cv=cv, param_grid=parameters_to_optimize[m], scoring='f1_weighted')\n",
    "                classifier = models[m]\n",
    "\n",
    "                classifier.fit(X_train, Y_train, train_lengths)# + train_lengths2)\n",
    "\n",
    "                Y_pred = [classifier.predict(X_test[i]) for i in xrange(len(X_test))]\n",
    "                #print Y_pred\n",
    "                confusion = confusion_matrix(Y_test, Y_pred)\n",
    "                print \"confusion matrix:\\n\", confusion\n",
    "                conf = f1_from_confusion(confusion)\n",
    "                conf_all = np.average(conf, axis=0)\n",
    "                print conf_all\n",
    "\n",
    "\n",
    "                r[n,m] = conf_all[0]\n",
    "                p[n,m] = conf_all[1]\n",
    "                f1[n,m] = conf_all[2]\n",
    "                n += 1\n",
    "\n",
    "                progress.value += 1\n",
    "        return r, p, f1\n",
    "            \n",
    "            \n",
    "        \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    models = [MultipleHMM(GaussianHMM(n_components=12, covariance_type='diag', n_iter=50))]\n",
    "\n",
    "    recall, precision, f1_score = model_comparison(features, label_to_numbers(labels, dataset_class_histogram(dataset)),features2, label_to_numbers(labels2, dataset_class_histogram(dataset2)), features3, label_to_numbers(labels3, dataset_class_histogram(dataset3)), models)\n",
    "\n",
    "    print \"Teste: \", bases_teste[rev], \"\\nrecal:\",recall, \"\\nprecision:\", precision, \"\\nf1_score:\", f1_score, \"\\n\"\n",
    "    print \"media_recall:\", np.average(recall, axis=0), \"media_precision:\", np.average(precision, axis=0), \"media_f1_score:\", np.average(f1_score, axis=0), \"\\n\"\n",
    "\n",
    "    sigma = np.std(f1_score)\n",
    "    print \"Desvio Padrao:\", sigma, \"\\n\"\n",
    "\n",
    "\n",
    "    if len(models) > 2:\n",
    "    #    print [f1_score[:,i].T for i in range(len(models))]\n",
    "        print \"Anova: \", f_oneway( *f1_score.T  )[1]\n",
    "\n",
    "    elif len(models) == 2:\n",
    "        print \"T-test:\", ttest( f1_score[:,0].T,  f1_score[:,1].T)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
