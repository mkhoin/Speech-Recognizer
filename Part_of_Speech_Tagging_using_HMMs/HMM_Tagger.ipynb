{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Part of Speech Tagging with Hidden Markov Models \n",
    "---\n",
    "### Introduction\n",
    "\n",
    "Part of speech tagging is the process of determining the syntactic category of a word from the words in its surrounding context. It is often used to help disambiguate natural language phrases because it can be done quickly with high accuracy. Tagging can be used for many NLP tasks like determining correct pronunciation during speech synthesis (for example, _dis_-count as a noun vs dis-_count_ as a verb), for information retrieval, and for word sense disambiguation.\n",
    "\n",
    "In this notebook, we explore the Pomegranate API and use the [Pomegranate](http://pomegranate.readthedocs.io/) library to build a hidden Markov model for part of speech tagging using a \"universal\" tagset. Hidden Markov models have been able to achieve [>96% tag accuracy with larger tagsets on realistic text corpora](http://www.coli.uni-saarland.de/~thorsten/publications/Brants-ANLP00.pdf). Hidden Markov models have also been used for speech recognition and speech generation, machine translation, gene recognition for bioinformatics, and human gesture recognition for computer vision, and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Road Ahead\n",
    "\n",
    "- [Step 0](#Step-0:-Build-a-Simple-HMM-model-and-explore-the-Pomegranate-library): Build a Simple HMM model and explore the Pomegranate library\n",
    "- [Step 1](#Step-1:-Read-and-preprocess-the-dataset): Review the provided interface to load and access the text corpus\n",
    "- [Step 2](#Step-2:-Build-a-Most-Frequent-Class-tagger): Build a Most Frequent Class tagger to use as a baseline\n",
    "- [Step 3](#Step-3:-Build-an-HMM-tagger): Build an HMM Part of Speech tagger and compare to the MFC baseline\n",
    "- [Step 4](#Step-4:-[Optional]-Improving-model-performance): (Optional) Improve the HMM tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter \"magic methods\" -- only need to be run once per kernel restart\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python modules -- this cell needs to be run again if you make changes to any of the files\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from helpers import show_model, Dataset\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple HMM\n",
    "---\n",
    "We start by building a simple HMM network based on an example from the textbook [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/).\n",
    "\n",
    "> You are the security guard stationed at a secret under-ground installation. Each day, you try to guess whether itâ€™s raining today, but your only access to the outside world occurs each morning when you see the director coming in with, or without, an umbrella.\n",
    "\n",
    "A simplified diagram of the required network topology is shown below.\n",
    "\n",
    "![](_example.png)\n",
    "\n",
    "### Describing the Network\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "$\\lambda = (A, B)$ specifies a Hidden Markov Model in terms of an emission probability distribution $A$ and a state transition probability distribution $B$.\n",
    "</div>\n",
    "\n",
    "HMM networks are parameterized by two distributions: the emission probabilties giving the conditional probability of observing evidence values for each hidden state, and the transition probabilities giving the conditional probability of moving between states during the sequence. Additionally, we can specify an initial distribution describing the probability of a sequence starting in each state.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "At each time $t$, $X_t$ represents the hidden state, and $Y_t$ represents an observation at that time.\n",
    "</div>\n",
    "\n",
    "In this problem, $t$ corresponds to each day of the week and the hidden state represent the weather outside (whether it is Rainy or Sunny) and observations record whether the security guard sees the director carrying an umbrella or not.\n",
    "\n",
    "For example, during some particular week the guard may observe an umbrella ['yes', 'no', 'yes', 'no', 'yes'] on Monday-Friday, while the weather outside is ['Rainy', 'Sunny', 'Sunny', 'Sunny', 'Rainy']. In that case, $t=Wednesday$, $Y_{Wednesday}=yes$, and $X_{Wednesday}=Sunny$. (It might be surprising that the guard would observe an umbrella on a sunny day, but it is possible under this type of model.)\n",
    "\n",
    "### Initializing an HMM Network with Pomegranate\n",
    "The Pomegranate library supports [two initialization methods](http://pomegranate.readthedocs.io/en/latest/HiddenMarkovModel.html#initialization). We can either explicitly provide the three distributions, or we can build the network line-by-line. We'll use the line-by-line method for the example network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the HMM model\n",
    "model = HiddenMarkovModel(name=\"Example Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the Hidden States\n",
    "When the HMM model is specified line-by-line, the object starts as an empty container. The first step is to name each state and attach an emission distribution.\n",
    "\n",
    "#### Observation Emission Probabilities: $P(Y_t | X_t)$\n",
    "We need to assume that we have some prior knowledge (possibly from a data set) about the director's behavior to estimate the emission probabilities for each hidden state. In real problems we often estimate the emission probabilities empirically, which is what we'll do for the part of speech tagger. Our imaginary data will produce the conditional probability table below. (Note that the rows sum to 1.0)\n",
    "\n",
    "| |  $yes$  | $no$ |\n",
    "| --- | --- | --- |\n",
    "| $Sunny$ |   0.10  | 0.90 |\n",
    "| $Rainy$ | 0.80 | 0.20 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# create the HMM model\n",
    "model = HiddenMarkovModel(name=\"Example Model\")\n",
    "\n",
    "# emission probability distributions, P(umbrella | weather)\n",
    "sunny_emissions = DiscreteDistribution({\"yes\": 0.1, \"no\": 0.9})\n",
    "sunny_state = State(sunny_emissions, name=\"Sunny\")\n",
    "\n",
    "#create a discrete distribution for the rainy emissions from the probability table\n",
    "# above & use that distribution to create a state named Rainy\n",
    "rainy_emissions = DiscreteDistribution({\"yes\": 0.8, \"no\": 0.2})\n",
    "rainy_state = State(rainy_emissions, name=\"Rainy\")\n",
    "\n",
    "# add the states to the model\n",
    "model.add_states(sunny_state, rainy_state)\n",
    "\n",
    "assert rainy_emissions.probability(\"yes\") == 0.8, \"The director brings his umbrella with probability 0.8 on rainy days\"\n",
    "print(\"Looks good so far!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding Transitions\n",
    "Once the states are added to the model, we can build up the desired topology of individual state transitions.\n",
    "\n",
    "#### Initial Probability $P(X_0)$:\n",
    "We will assume that we don't know anything useful about the likelihood of a sequence starting in either state. If the sequences start each week on Monday and end each week on Friday (so each week is a new sequence), then this assumption means that it's equally likely that the weather on a Monday may be Rainy or Sunny. We can assign equal probability to each starting state by setting $P(X_0=Rainy) = 0.5$ and $P(X_0=Sunny)=0.5$:\n",
    "\n",
    "| $Sunny$ | $Rainy$ |\n",
    "| --- | ---\n",
    "| 0.5 | 0.5 |\n",
    "\n",
    "#### State transition probabilities $P(X_{t} | X_{t-1})$\n",
    "Finally, we will assume for this example that we can estimate transition probabilities from something like historical weather data for the area. In real problems we can often use the structure of the problem (like a language grammar) to impose restrictions on the transition probabilities, then re-estimate the parameters with the same training data used to estimate the emission probabilities. Under this assumption, we get the conditional probability table below. (Note that the rows sum to 1.0)\n",
    "\n",
    "| | $Sunny$ | $Rainy$ |\n",
    "| --- | --- | --- |\n",
    "|$Sunny$| 0.80 | 0.20 |\n",
    "|$Rainy$| 0.40 | 0.60 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! You've finished the model.\n"
     ]
    }
   ],
   "source": [
    "# create edges for each possible state transition in the model\n",
    "# equal probability of a sequence starting on either a rainy or sunny day\n",
    "model.add_transition(model.start, sunny_state, 0.5)\n",
    "model.add_transition(model.start, rainy_state, 0.5)\n",
    "\n",
    "# add sunny day transitions (we already know estimates of these probabilities\n",
    "# from the problem statement)\n",
    "model.add_transition(sunny_state, sunny_state, 0.8)  # 80% sunny->sunny\n",
    "model.add_transition(sunny_state, rainy_state, 0.2)  # 20% sunny->rainy\n",
    "\n",
    "# add rainy day transitions using the probabilities specified in the transition table\n",
    "model.add_transition(rainy_state, sunny_state, 0.4)  # 40% rainy->sunny\n",
    "model.add_transition(rainy_state, rainy_state, 0.6)  # 60% rainy->rainy\n",
    "\n",
    "# finally, call the .bake() method to finalize the model\n",
    "model.bake()\n",
    "\n",
    "assert model.edge_count() == 6, \"There should be two edges from model.start, two from Rainy, and two from Sunny\"\n",
    "assert model.node_count() == 4, \"The states should include model.start, model.end, Rainy, and Sunny\"\n",
    "print(\"Great! You've finished the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Network\n",
    "---\n",
    "We have provided a helper function called `show_model()` that generates a PNG image from a Pomegranate HMM network. We can specify an optional filename to save the file to disk. Setting the \"show_ends\" argument True will add the model start & end states that are included in every Pomegranate network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd52822fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_model(model, figsize=(5, 5), filename=\"example.png\", overwrite=True, show_ends=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Model\n",
    "The states of the model can be accessed using array syntax on the `HMM.states` attribute, and the transition matrix can be accessed by calling `HMM.dense_transition_matrix()`. Element $(i, j)$ encodes the probability of transitioning from state $i$ to state $j$. For example, with the default column order specified, element $(2, 1)$ gives the probability of transitioning from \"Rainy\" to \"Sunny\", which we specified as 0.4.\n",
    "\n",
    "Run the next cell to inspect the full state transition matrix, then read the . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state transition matrix, P(Xt|Xt-1):\n",
      "\n",
      "[[ 0.   0.5  0.5  0. ]\n",
      " [ 0.   0.8  0.2  0. ]\n",
      " [ 0.   0.4  0.6  0. ]\n",
      " [ 0.   0.   0.   0. ]]\n",
      "\n",
      "The transition probability from Rainy to Sunny is 40%\n"
     ]
    }
   ],
   "source": [
    "column_order = [\"Example Model-start\", \"Sunny\", \"Rainy\", \"Example Model-end\"]  # Override the Pomegranate default order\n",
    "column_names = [s.name for s in model.states]\n",
    "order_index = [column_names.index(c) for c in column_order]\n",
    "\n",
    "# re-order the rows/columns to match the specified column order\n",
    "transitions = model.dense_transition_matrix()[:, order_index][order_index, :]\n",
    "print(\"The state transition matrix, P(Xt|Xt-1):\\n\")\n",
    "print(transitions)\n",
    "print(\"\\nThe transition probability from Rainy to Sunny is {:.0f}%\".format(100 * transitions[2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in Hidden Markov Models\n",
    "---\n",
    "Before moving on, we'll use this simple network to quickly go over the Pomegranate API to perform the three most common HMM tasks:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Likelihood Evaluation**<br>\n",
    "Given a model $\\lambda=(A,B)$ and a set of observations $Y$, determine $P(Y|\\lambda)$, the likelihood of observing that sequence from the model\n",
    "</div>\n",
    "\n",
    "We can use the weather prediction model to evaluate the likelihood of the sequence [yes, yes, yes, yes, yes] (or any other state sequence). The likelihood is often used in problems like machine translation to weight interpretations in conjunction with a statistical language model.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Hidden State Decoding**<br>\n",
    "Given a model $\\lambda=(A,B)$ and a set of observations $Y$, determine $Q$, the most likely sequence of hidden states in the model to produce the observations\n",
    "</div>\n",
    "\n",
    "We can use the weather prediction model to determine the most likely sequence of Rainy/Sunny states for a known observation sequence, like [yes, no] -> [Rainy, Sunny]. We will use decoding in the part of speech tagger to determine the tag for each word of a sentence. The decoding can be further split into \"smoothing\" when we want to calculate past states, \"filtering\" when we want to calculate the current state, or \"prediction\" if we want to calculate future states. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Parameter Learning**<br>\n",
    "Given a model topography (set of states and connections) and a set of observations $Y$, learn the transition probabilities $A$ and emission probabilities $B$ of the model, $\\lambda=(A,B)$\n",
    "</div>\n",
    "\n",
    "We don't need to learn the model parameters for the weather problem or POS tagging, but it is supported by Pomegranate.\n",
    "\n",
    "### Calculate Sequence Likelihood\n",
    "\n",
    "Calculating the likelihood of an observation sequence from an HMM network is performed with the [forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm). Pomegranate provides the the `HMM.forward()` method to calculate the full matrix showing the likelihood of aligning each observation to each state in the HMM, and the `HMM.log_probability()` method to calculate the cumulative likelihood over all possible hidden state paths that the specified model generated the observation sequence.\n",
    "\n",
    "Fill in the code in the next section with a sample observation sequence and then use the `forward()` and `log_probability()` methods to evaluate the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Rainy      Sunny      Example Model-start      Example Model-end   \n",
      " <start>      0%         0%               100%                     0%          \n",
      "   yes       40%         5%                0%                      0%          \n",
      "    no        5%        18%                0%                      0%          \n",
      "   yes        5%         2%                0%                      0%          \n",
      "\n",
      "The likelihood over all possible paths of this model producing the sequence ['yes', 'no', 'yes'] is 6.92%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input a sequence of 'yes'/'no' values in the list below for testing\n",
    "observations = ['yes', 'no', 'yes']\n",
    "\n",
    "assert len(observations) > 0, \"You need to choose a sequence of 'yes'/'no' observations to test\"\n",
    "\n",
    "# use model.forward() to calculate the forward matrix of the observed sequence,\n",
    "# and then use np.exp() to convert from log-likelihood to likelihood\n",
    "forward_matrix = np.exp(model.forward(observations))\n",
    "\n",
    "# use model.log_probability() to calculate the all-paths likelihood of the\n",
    "# observed sequence and then use np.exp() to convert log-likelihood to likelihood\n",
    "probability_percentage = np.exp(model.log_probability(observations))\n",
    "\n",
    "# Display the forward probabilities\n",
    "print(\"         \" + \"\".join(s.name.center(len(s.name)+6) for s in model.states))\n",
    "for i in range(len(observations) + 1):\n",
    "    print(\" <start> \" if i==0 else observations[i - 1].center(9), end=\"\")\n",
    "    print(\"\".join(\"{:.0f}%\".format(100 * forward_matrix[i, j]).center(len(s.name) + 6)\n",
    "                  for j, s in enumerate(model.states)))\n",
    "\n",
    "print(\"\\nThe likelihood over all possible paths \" + \\\n",
    "      \"of this model producing the sequence {} is {:.2f}%\\n\\n\"\n",
    "      .format(observations, 100 * probability_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the Most Likely Hidden State Sequence\n",
    "\n",
    "The [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) calculates the single path with the highest likelihood to produce a specific observation sequence. Pomegranate provides the `HMM.viterbi()` method to calculate both the hidden state sequence and the corresponding likelihood of the viterbi path.\n",
    "\n",
    "This is called \"decoding\" because we use the observation sequence to decode the corresponding hidden state sequence. In the part of speech tagging problem, the hidden states map to parts of speech and the observations map to sentences. Given a sentence, Viterbi decoding finds the most likely sequence of part of speech tags corresponding to the sentence.\n",
    "\n",
    "We use the same sample observation sequence as used above, and then use the `model.viterbi()` method to calculate the likelihood and most likely state sequence. We compare the Viterbi likelihood against the forward algorithm likelihood for the observation sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely weather sequence to have generated these observations is ['Rainy', 'Sunny', 'Rainy'] at 2.30%.\n"
     ]
    }
   ],
   "source": [
    "# input a sequence of 'yes'/'no' values in the list below for testing\n",
    "observations = ['yes', 'no', 'yes']\n",
    "\n",
    "# use model.viterbi to find the sequence likelihood & the most likely path\n",
    "viterbi_likelihood, viterbi_path = model.viterbi(observations)\n",
    "\n",
    "print(\"The most likely weather sequence to have generated \" + \\\n",
    "      \"these observations is {} at {:.2f}%.\"\n",
    "      .format([s[1].name for s in viterbi_path[1:]], np.exp(viterbi_likelihood)*100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward likelihood vs Viterbi likelihood\n",
    "Run the cells below to see the likelihood of each sequence of observations with length 3, and compare with the viterbi path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The likelihood of observing ['no', 'no', 'yes'] if the weather sequence is...\n",
      "\t('Sunny', 'Sunny', 'Sunny') is 2.59% \n",
      "\t('Sunny', 'Sunny', 'Rainy') is 5.18%  <-- Viterbi path\n",
      "\t('Sunny', 'Rainy', 'Sunny') is 0.07% \n",
      "\t('Sunny', 'Rainy', 'Rainy') is 0.86% \n",
      "\t('Rainy', 'Sunny', 'Sunny') is 0.29% \n",
      "\t('Rainy', 'Sunny', 'Rainy') is 0.58% \n",
      "\t('Rainy', 'Rainy', 'Sunny') is 0.05% \n",
      "\t('Rainy', 'Rainy', 'Rainy') is 0.58% \n",
      "\n",
      "The total likelihood of observing ['no', 'no', 'yes'] over all possible paths is 10.20%\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "observations = ['no', 'no', 'yes']\n",
    "\n",
    "p = {'Sunny': {'Sunny': np.log(.8), 'Rainy': np.log(.2)}, 'Rainy': {'Sunny': np.log(.4), 'Rainy': np.log(.6)}}\n",
    "e = {'Sunny': {'yes': np.log(.1), 'no': np.log(.9)}, 'Rainy':{'yes':np.log(.8), 'no':np.log(.2)}}\n",
    "o = observations\n",
    "k = []\n",
    "vprob = np.exp(model.viterbi(o)[0])\n",
    "print(\"The likelihood of observing {} if the weather sequence is...\".format(o))\n",
    "for s in product(*[['Sunny', 'Rainy']]*3):\n",
    "    k.append(np.exp(np.log(.5)+e[s[0]][o[0]] + p[s[0]][s[1]] + e[s[1]][o[1]] + p[s[1]][s[2]] + e[s[2]][o[2]]))\n",
    "    print(\"\\t{} is {:.2f}% {}\".format(s, 100 * k[-1], \" <-- Viterbi path\" if k[-1] == vprob else \"\"))\n",
    "print(\"\\nThe total likelihood of observing {} over all possible paths is {:.2f}%\".format(o, 100*sum(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read and preprocess the dataset\n",
    "---\n",
    "We'll start by reading in a text corpus and splitting it into a training and testing dataset. The data set is a copy of the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) (originally from the [NLTK](https://www.nltk.org/) library) that has already been pre-processed to only include the [universal tagset](https://arxiv.org/pdf/1104.2086.pdf). We should expect to get slightly higher accuracy using this simplified tagset than the same model would achieve on a larger tagset like the full [Penn treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), but the process we follow would be the same.\n",
    "\n",
    "The `Dataset` class provided in helpers.py will read and parse the corpus. We can generate our own datasets compatible with the reader by writing them to the following format. The dataset is stored in plaintext as a collection of words and corresponding tags. Each sentence starts with a unique identifier on the first line, followed by one tab-separated word/tag pair on each following line. Sentences are separated by a single blank line.\n",
    "\n",
    "Example from the Brown corpus. \n",
    "```\n",
    "b100-38532\n",
    "Perhaps\tADV\n",
    "it\tPRON\n",
    "was\tVERB\n",
    "right\tADJ\n",
    ";\t.\n",
    ";\t.\n",
    "\n",
    "b100-35577\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences in the corpus.\n",
      "There are 45872 sentences in the training set.\n",
      "There are 11468 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n",
    "\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset Interface\n",
    "\n",
    "We can access (mostly) immutable references to the dataset through a simple interface provided through the `Dataset` class, which represents an iterable collection of sentences along with easy access to partitions of the data for training & testing. \n",
    "\n",
    "```\n",
    "Dataset-only Attributes:\n",
    "    training_set - reference to a Subset object containing the samples for training\n",
    "    testing_set - reference to a Subset object containing the samples for testing\n",
    "\n",
    "Dataset & Subset Attributes:\n",
    "    sentences - a dictionary with an entry {sentence_key: Sentence()} for each sentence in the corpus\n",
    "    keys - an immutable ordered (not sorted) collection of the sentence_keys for the corpus\n",
    "    vocab - an immutable collection of the unique words in the corpus\n",
    "    tagset - an immutable collection of the unique tags in the corpus\n",
    "    X - returns an array of words grouped by sentences ((w11, w12, w13, ...), (w21, w22, w23, ...), ...)\n",
    "    Y - returns an array of tags grouped by sentences ((t11, t12, t13, ...), (t21, t22, t23, ...), ...)\n",
    "    N - returns the number of distinct samples (individual words or tags) in the dataset\n",
    "\n",
    "Methods:\n",
    "    stream() - returns an flat iterable over all (word, tag) pairs across all sentences in the corpus\n",
    "    __iter__() - returns an iterable over the data as (sentence_key, Sentence()) pairs\n",
    "    __len__() - returns the nubmer of sentences in the dataset\n",
    "```\n",
    "\n",
    "For example, consider a Subset, `subset`, of the sentences `{\"s0\": Sentence((\"See\", \"Spot\", \"run\"), (\"VERB\", \"NOUN\", \"VERB\")), \"s1\": Sentence((\"Spot\", \"ran\"), (\"NOUN\", \"VERB\"))}`. The subset will have these attributes:\n",
    "\n",
    "```\n",
    "subset.keys == {\"s1\", \"s0\"}  # unordered\n",
    "subset.vocab == {\"See\", \"run\", \"ran\", \"Spot\"}  # unordered\n",
    "subset.tagset == {\"VERB\", \"NOUN\"}  # unordered\n",
    "subset.X == ((\"Spot\", \"ran\"), (\"See\", \"Spot\", \"run\"))  # order matches .keys\n",
    "subset.Y == ((\"NOUN\", \"VERB\"), (\"VERB\", \"NOUN\", \"VERB\"))  # order matches .keys\n",
    "subset.N == 7  # there are a total of seven observations over all sentences\n",
    "len(subset) == 2  # because there are two sentences\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences\n",
    "\n",
    "`Dataset.sentences` is a dictionary of all sentences in the training corpus, each keyed to a unique sentence identifier. Each `Sentence` is itself an object with two attributes: a tuple of the words in the sentence named `words` and a tuple of the tag corresponding to each word named `tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: b100-38532\n",
      "words:\n",
      "\t('Perhaps', 'it', 'was', 'right', ';', ';')\n",
      "tags:\n",
      "\t('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')\n"
     ]
    }
   ],
   "source": [
    "key = 'b100-38532'\n",
    "print(\"Sentence: {}\".format(key))\n",
    "print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n",
    "print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**Note:** The underlying iterable sequence is **unordered** over the sentences in the corpus; it is not guaranteed to return the sentences in a consistent order between calls. We use `Dataset.stream()`, `Dataset.keys`, `Dataset.X`, or `Dataset.Y` attributes if we need ordered access to the data.\n",
    "</div>\n",
    "\n",
    "#### Counting Unique Elements\n",
    "\n",
    "We can access the list of unique words (the dataset vocabulary) via `Dataset.vocab` and the unique list of tags via `Dataset.tagset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1161192 samples of 56057 unique words in the corpus.\n",
      "There are 928458 samples of 50536 unique words in the training set.\n",
      "There are 232734 samples of 25112 unique words in the testing set.\n",
      "There are 5521 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "      .format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\"\n",
    "      .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "      .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing word and tag Sequences\n",
    "The `Dataset.X` and `Dataset.Y` attributes provide access to ordered collections of matching word and tag sequences for each sentence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\n",
      "\n",
      "Labels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "Sentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')\n",
      "\n",
      "Labels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accessing words with Dataset.X and tags with Dataset.Y \n",
    "for i in range(2):    \n",
    "    print(\"Sentence {}:\".format(i + 1), data.X[i])\n",
    "    print()\n",
    "    print(\"Labels {}:\".format(i + 1), data.Y[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing (word, tag) Samples\n",
    "The `Dataset.stream()` method returns an iterator that chains together every pair of (word, tag) entries across all sentences in the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t ('Mr.', 'NOUN')\n",
      "\t ('Podger', 'NOUN')\n",
      "\t ('had', 'VERB')\n",
      "\t ('thanked', 'VERB')\n",
      "\t ('him', 'PRON')\n",
      "\t ('gravely', 'ADV')\n",
      "\t (',', '.')\n"
     ]
    }
   ],
   "source": [
    "# use Dataset.stream() (word, tag) samples for the entire corpus\n",
    "print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "for i, pair in enumerate(data.stream()):\n",
    "    print(\"\\t\", pair)\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For both our baseline tagger and the HMM model we'll build, we need to estimate the frequency of tags & words from the frequency counts of observations in the training corpus. In the next several cells you will complete functions to compute the counts of several sets of counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build a Most Frequent Class tagger\n",
    "---\n",
    "\n",
    "Perhaps the simplest tagger (and a good baseline for tagger performance) is to simply choose the tag most frequently assigned to each word. This \"most frequent class\" tagger inspects each observed word in the sequence and assigns it the label that was most often assigned to that word in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Counts\n",
    "\n",
    "The function below computes the joint frequency counts for two input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def pair_counts(sequences_A, sequences_B):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the first sequence list\n",
    "    that counts the number of occurrences of the corresponding value from the\n",
    "    second sequences list.\n",
    "    \n",
    "    For example, if sequences_A is tags and sequences_B is the corresponding\n",
    "    words, then if 1244 sequences contain the word \"time\" tagged as a NOUN, then\n",
    "    you should return a dictionary such that pair_counts[NOUN][time] == 1244\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {}\n",
    "    index = 0;\n",
    "    for tag in sequences_A:\n",
    "        word = sequences_B[index]\n",
    "        if not tag in result:\n",
    "            result[tag] = {}\n",
    "        if not word in result[tag]:\n",
    "            result[tag][word]=0\n",
    "        result[tag][word]+=1\n",
    "        index+=1\n",
    "    return result \n",
    "\n",
    "\n",
    "# Calculate C(t_i, w_i)\n",
    "tags = [tag for _, tag in data.training_set.stream()]\n",
    "words = [word for word, _ in data.training_set.stream()]\n",
    "emission_counts = pair_counts(tags,words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Class Tagger\n",
    "\n",
    "We use `pair_counts()` function and the training dataset to find the most frequent class label for each word in the training data, and populate the `mfc_table` below. The table keys should be words, and the values should be the appropriate tag string.\n",
    "\n",
    "The `MFCTagger` class is provided to mock the interface of Pomegranite HMM models so that they can be used interchangeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup table mfc_table where mfc_table[word] contains the tag label most frequently assigned to that word\n",
    "from collections import namedtuple\n",
    "\n",
    "FakeState = namedtuple(\"FakeState\", \"name\")\n",
    "\n",
    "class MFCTagger:\n",
    "    missing = FakeState(name=\"<MISSING>\")\n",
    "    \n",
    "    def __init__(self, table):\n",
    "        self.table = defaultdict(lambda: MFCTagger.missing)\n",
    "        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n",
    "        \n",
    "    def viterbi(self, seq):\n",
    "        \"\"\"This method simplifies predictions by matching the Pomegranate viterbi() interface\"\"\"\n",
    "        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))\n",
    "\n",
    "\n",
    "# calculate the frequency of each tag being assigned to each word (similar, but not\n",
    "# the same as the emission probabilities) and use it to fill the mfc_table\n",
    "\n",
    "word_counts = pair_counts(words, tags)\n",
    "\n",
    "mfc_table = {}\n",
    "for word, tag_set in word_counts.items():\n",
    "    #find tag with maximum value\n",
    "    current_max = 0\n",
    "    for tag_name, tag_count in tag_set.items():\n",
    "        if tag_count > current_max:\n",
    "            current_max = tag_count\n",
    "            tag_with_max_count = tag_name\n",
    "    mfc_table[word] = tag_with_max_count\n",
    "\n",
    "mfc_model = MFCTagger(mfc_table) # Create a Most Frequent Class tagger instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with a Model\n",
    "The helper functions provided below interface with Pomegranate network models & the mocked MFCTagger to take advantage of the [missing value](http://pomegranate.readthedocs.io/en/latest/nan.html) functionality in Pomegranate through a simple sequence decoding function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n",
    "    by the literal string value 'nan'. Pomegranate will ignore these values\n",
    "    during computation.\n",
    "    \"\"\"\n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Decoding Sequences with MFC Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-23146\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-35462\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', '<MISSING>', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADV', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:3]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, mfc_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Accuracy\n",
    "\n",
    "The function below will evaluate the accuracy of the MFC tagger on the collection of all sentences from a text corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\n",
    "    in the input X and comparing the prediction with the true labels in Y.\n",
    "    \n",
    "    The X should be an array whose first dimension is the number of sentences to test,\n",
    "    and each element of the array should be an iterable of the words in the sequence.\n",
    "    The arrays X and Y should have the exact same shape.\n",
    "    \n",
    "    X = [(\"See\", \"Spot\", \"run\"), (\"Run\", \"Spot\", \"run\", \"fast\"), ...]\n",
    "    Y = [(), (), ...]\n",
    "    \"\"\"\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        \n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the accuracy of the MFC tagger\n",
    "We evaluate the accuracy of the tagger on the training and test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy mfc_model: 95.72%\n",
      "testing accuracy mfc_model: 93.01%\n"
     ]
    }
   ],
   "source": [
    "mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n",
    "print(\"training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\n",
    "\n",
    "mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n",
    "print(\"testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build an HMM tagger\n",
    "---\n",
    "The HMM tagger has one hidden state for each possible tag, and parameterized by two distributions: the emission probabilties giving the conditional probability of observing a given **word** from each hidden state, and the transition probabilities giving the conditional probability of moving between **tags** during the sequence.\n",
    "\n",
    "We will also estimate the starting probability distribution (the probability of each **tag** being the first tag in a sequence), and the terminal probability distribution (the probability of each **tag** being the last tag in a sequence).\n",
    "\n",
    "The maximum likelihood estimate of these distributions can be calculated from the frequency counts as described in the following sections where you'll implement functions to count the frequencies, and finally build the model. The HMM model will make predictions according to the formula:\n",
    "\n",
    "$$t_i^n = \\underset{t_i^n}{\\mathrm{argmin}} \\prod_{i=1}^n P(w_i|t_i) P(t_i|t_{i-1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Counts\n",
    "\n",
    "Below we estimate the co-occurrence frequency of each symbol over all of the input sequences. The unigram probabilities in our HMM model are estimated from the formula below, where N is the total number of samples in the input. (You only need to compute the counts for now.)\n",
    "\n",
    "$$P(tag_1) = \\frac{C(tag_1)}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequence list that\n",
    "    counts the number of occurrences of the value in the sequences list. The sequences\n",
    "    collection should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the tag NOUN appears 275558 times over all the input sequences,\n",
    "    then you should return a dictionary such that your_unigram_counts[NOUN] == 275558.\n",
    "    \"\"\"\n",
    "    # Finish this function!\n",
    "    result = {}\n",
    "    for sequence in sequences:\n",
    "        for tag in sequence:\n",
    "            if not tag in result:\n",
    "                result[tag]=0\n",
    "            result[tag]+=1\n",
    "    return result\n",
    "\n",
    "# call unigram_counts with a list of tag sequences from the training set\n",
    "tag_unigrams = unigram_counts(data.training_set.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Counts\n",
    "\n",
    "We estimate the co-occurrence frequency of each pair of symbols in each of the input sequences. These counts are used in the HMM model to estimate the bigram probability of two tags from the frequency counts according to the formula: $$P(tag_2|tag_1) = \\frac{C(tag_2|tag_1)}{C(tag_2)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n",
    "    list that counts the number of occurrences of pair in the sequences list. The input\n",
    "    should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n",
    "    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n",
    "    \"\"\"\n",
    "\n",
    "    # Finish this function!\n",
    "    result = {}\n",
    "    for sequence in sequences:\n",
    "        if len(sequence) > 1:\n",
    "            for idx in range(1,len(sequence)):\n",
    "                bigram = (sequence[idx-1],sequence[idx])\n",
    "                if not bigram in result:\n",
    "                    result[bigram]=0\n",
    "                result[bigram]+=1\n",
    "    return result\n",
    "\n",
    "# call bigram_counts with a list of tag sequences from the training set\n",
    "tag_bigrams = bigram_counts(data.training_set.Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Starting Counts\n",
    "We estimate the bigram probabilities of a sequence starting with each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the beginning of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 8093 sequences start with NOUN, then you should return a\n",
    "    dictionary such that your_starting_counts[NOUN] == 8093\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function!\n",
    "    result = {}\n",
    "    for sequence in sequences:\n",
    "        start_tag = sequence[0]\n",
    "        if not start_tag in result:\n",
    "            result[start_tag]=0\n",
    "        result[start_tag]+=1\n",
    "    return result\n",
    "\n",
    "# Calculate the count of each tag starting a sequence\n",
    "tag_starts = starting_counts(data.training_set.Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Sequence Ending Counts\n",
    "Complete the function below to estimate the bigram probabilities of a sequence ending with each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the end of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 18 sequences end with DET, then you should return a\n",
    "    dictionary such that your_starting_counts[DET] == 18\n",
    "    \"\"\"\n",
    "    # Finish this function!\n",
    "    result = {}\n",
    "    for sequence in sequences:\n",
    "        end_tag = sequence[-1]\n",
    "        if not end_tag in result:\n",
    "            result[end_tag]=0\n",
    "        result[end_tag]+=1\n",
    "    return result\n",
    "\n",
    "# Calculate the count of each tag ending a sequence\n",
    "tag_ends = ending_counts(data.training_set.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic HMM Tagger\n",
    "We use the tag unigrams and bigrams calculated above to construct a hidden Markov tagger.\n",
    "\n",
    "- Add one state per tag\n",
    "    - The emission distribution at each state should be estimated with the formula: $P(w|t) = \\frac{C(t, w)}{C(t)}$\n",
    "- Add an edge from the starting state `basic_model.start` to each tag\n",
    "    - The transition probability should be estimated with the formula: $P(t|start) = \\frac{C(start, t)}{C(start)}$\n",
    "- Add an edge from each tag to the end state `basic_model.end`\n",
    "    - The transition probability should be estimated with the formula: $P(end|t) = \\frac{C(t, end)}{C(t)}$\n",
    "- Add an edge between _every_ pair of tags\n",
    "    - The transition probability should be estimated with the formula: $P(t_2|t_1) = \\frac{C(t_1, t_2)}{C(t_1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "# create states with emission probability distributions P(word | tag) and add to the model\n",
    "# We add loop & create/add new states)\n",
    "states = {}\n",
    "# calculate outside of loop for efficiency\n",
    "sum_of_start_values_counts = sum(tag_starts.values())\n",
    "sum_of_end_values_counts = sum(tag_ends.values())\n",
    "\n",
    "for tag in data.tagset:\n",
    "    emission_probabilities = {word: count / tag_unigrams[tag] for word, count in emission_counts[tag].items()}\n",
    "    tag_distribution = DiscreteDistribution(emission_probabilities)\n",
    "    s=State(tag_distribution, name=tag)\n",
    "    states[tag]=s\n",
    "    basic_model.add_state(s)\n",
    "    # add the start and end transitions / probabilities\n",
    "    start_proba = tag_starts[tag]/sum_of_start_values_counts\n",
    "    end_proba = tag_ends[tag]/sum_of_end_values_counts\n",
    "    basic_model.add_transition(basic_model.start, s, start_proba)\n",
    "    basic_model.add_transition(s, basic_model.end, end_proba)\n",
    "\n",
    "\n",
    "# add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n",
    "# we add loop & add transitions\n",
    "for tag1 in data.tagset:\n",
    "    tag1_state = states[tag1]\n",
    "    # prob_sum of all transition probabilities must be near to 1.0 at the end (assert / check)\n",
    "    prob_sum = 0\n",
    "    for tag2 in data.tagset:\n",
    "        tag2_state = states[tag2]\n",
    "        bigram = (tag1,tag2)\n",
    "        transition_probability = tag_bigrams[bigram]/tag_unigrams[tag1]\n",
    "        prob_sum+=transition_probability\n",
    "        basic_model.add_transition(tag1_state, tag2_state, transition_probability)\n",
    "    # check sum of transition probabilities. \".\" is allowed to have lower probability sum because it is often followed by end state\n",
    "    if tag1 != '.':\n",
    "        assert prob_sum > 0.994 and prob_sum <= 1.0, \"transition probabilities of \"+tag1+\" do not sum up to ~1.0: \"+str(prob_sum)\n",
    "        \n",
    "# finalize the model\n",
    "basic_model.bake()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy basic hmm model: 97.54%\n",
      "testing accuracy basic hmm model: 95.96%\n"
     ]
    }
   ],
   "source": [
    "hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)\n",
    "print(\"training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)\n",
    "print(\"testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Decoding Sequences with the HMM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-23146\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-35462\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:3]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, basic_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Future Work: Improving model performance\n",
    "---\n",
    "There are additional enhancements that can be incorporated into your tagger that improve performance on larger tagsets where the data sparsity problem is more significant. The data sparsity problem arises because the same amount of data split over more tags means there will be fewer samples in each tag, and there will be more missing data  tags that have zero occurrences in the data. The techniques that can be employed are\"\n",
    "\n",
    "- [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) (pseudocounts)\n",
    "    Laplace smoothing is a technique where you add a small, non-zero value to all observed counts to offset for unobserved values.\n",
    "\n",
    "- Backoff Smoothing\n",
    "    Another smoothing technique is to interpolate between n-grams for missing data. This method is more effective than Laplace smoothing at combatting the data sparsity problem. Refer to chapters 4, 9, and 10 of the [Speech & Language Processing](https://web.stanford.edu/~jurafsky/slp3/) book for more information.\n",
    "\n",
    "- Extending to Trigrams\n",
    "    HMM taggers have achieved better than 96% accuracy on this dataset with the full Penn treebank tagset using an architecture described in [this](http://www.coli.uni-saarland.de/~thorsten/publications/Brants-ANLP00.pdf) paper. Altering your HMM to achieve the same performance would require implementing deleted interpolation (described in the paper), incorporating trigram probabilities in your frequency tables, and re-implementing the Viterbi algorithm to consider three consecutive states instead of two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
