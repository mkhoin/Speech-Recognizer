{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "from math import log\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "incrementer = 0.000000001\n",
    "\n",
    "def getFileContents(filename):\n",
    "    data = None\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    return data\n",
    "\n",
    "\n",
    "def getFileFromCommandLine():\n",
    "    filename = sys.argv[1]\n",
    "    return getFileContents(filename)\n",
    "\n",
    "\n",
    "def splitWordTag(word_tag_pair):\n",
    "    splitted = word_tag_pair.split('/')\n",
    "    tag = splitted[-1]\n",
    "    word = '/'.join(splitted[:-1])\n",
    "    return word, tag\n",
    "\n",
    "\n",
    "def getUniqueTags(tagged_data):\n",
    "    tags = {}\n",
    "    for line in tagged_data:\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            if tag in tags.keys():\n",
    "                tags[tag] += 1\n",
    "            else:\n",
    "                tags[tag] = 1\n",
    "    return tags\n",
    "\n",
    "\n",
    "def getOpenProbabilities(tagged_data, all_tags_dict):\n",
    "    global incrementer\n",
    "    sentences_count = len(tagged_data)\n",
    "    open_tag_count_dict = {}\n",
    "    for line in tagged_data:\n",
    "        first_word_tag_pairs = line.strip().split(' ')[0]\n",
    "        word, tag = splitWordTag(first_word_tag_pairs)\n",
    "        if tag in open_tag_count_dict.keys():\n",
    "            open_tag_count_dict[tag] += 1\n",
    "        else:\n",
    "            open_tag_count_dict[tag] = 1\n",
    "    \n",
    "    #increment all existing tags count to one\n",
    "    open_tag_count_dict.update((tag, occurances + incrementer) for tag, occurances in open_tag_count_dict.items())\n",
    "    sentences_count += (sentences_count*incrementer)\n",
    "    \n",
    "    #add one to non-opening tags\n",
    "    for tag in all_tags_dict.keys():\n",
    "        try:\n",
    "            val = open_tag_count_dict[tag]\n",
    "        except KeyError as e:\n",
    "            open_tag_count_dict[tag] = incrementer\n",
    "            sentences_count += incrementer\n",
    "    \n",
    "    open_tag_count_dict.update((tag, (occurances*1.0)/sentences_count) for tag, occurances in open_tag_count_dict.items())\n",
    "    return open_tag_count_dict\n",
    "\n",
    "\n",
    "def getCloseProbabilities(tagged_data, all_tags_dict):\n",
    "    global incrementer\n",
    "    sentences_count = len(tagged_data)\n",
    "    close_tag_count_dict = {}\n",
    "    for line in tagged_data:\n",
    "        last_word_tag_pairs = line.strip().split(' ')[-1]\n",
    "        word, tag = splitWordTag(last_word_tag_pairs)\n",
    "        if tag in close_tag_count_dict.keys():\n",
    "            close_tag_count_dict[tag] += 1\n",
    "        else:\n",
    "            close_tag_count_dict[tag] = 1\n",
    "            \n",
    "    #increment all existing tags count by one\n",
    "    close_tag_count_dict.update((tag, occurances + incrementer) for tag, occurances in close_tag_count_dict.items())\n",
    "    \n",
    "    sentences_count += (sentences_count*incrementer)\n",
    "    \n",
    "    #add one to non-closing tags\n",
    "    for tag in all_tags_dict.keys():\n",
    "        try:\n",
    "            val = close_tag_count_dict[tag]\n",
    "        except KeyError as e:\n",
    "            close_tag_count_dict[tag] = incrementer\n",
    "            sentences_count += incrementer\n",
    "            \n",
    "    close_tag_count_dict.update((tag, (occurances*1.0)/sentences_count) for tag, occurances in close_tag_count_dict.items())\n",
    "    return close_tag_count_dict\n",
    "\n",
    "\n",
    "def buildTransitionMatrix(tagged_data, tags_dict):\n",
    "    global incrementer\n",
    "    tags = tags_dict.keys()\n",
    "    tags.sort()\n",
    "    \n",
    "    tags_index_dict = {}\n",
    "    tags_index_dict_reverse = {}\n",
    "    for index, tag in enumerate(tags):\n",
    "        tags_index_dict[tag] = index\n",
    "        tags_index_dict_reverse[index] = tag\n",
    "    \n",
    "    tag_count = len(tags)\n",
    "    \n",
    "    feature_tags = {'PAGE_SEP' : [], 'URLS' : [], 'NUMERICS' : []}\n",
    "    feature_counts = {'PAGE_SEP' : 0, 'URLS' : 0, 'NUMERICS' : 0}\n",
    "    \n",
    "    #Change this line to np.ones for add 1 smoothing\n",
    "    transition_matrix = np.zeros(shape=(tag_count, tag_count))\n",
    "    \n",
    "    for line in tagged_data:\n",
    "        prev_tag = None\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        \n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            \n",
    "            if word.count('=') > 10 or word.count('_') > 10 or word.count('*') > 10 or word.count('-') > 10 or word.count('+') > 10:\n",
    "                feature_tags['PAGE_SEP'].append(tag)\n",
    "                feature_counts['PAGE_SEP'] += 1\n",
    "                \n",
    "            elif any(word.lower().endswith(last) for last in ('.com', '.net', '.org', '.edu')) or word.startswith('http') or word.startswith('www.'):\n",
    "                feature_tags['URLS'].append(tag)\n",
    "                feature_counts['URLS'] += 1\n",
    "                \n",
    "            elif [char.isdigit() for char in word].count(True) * 1.0 > len(word) * 0.4:\n",
    "                feature_tags['NUMERICS'].append(tag)\n",
    "                feature_counts['NUMERICS'] += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if prev_tag is not None:\n",
    "                transition_matrix[tags_index_dict[prev_tag]][tags_index_dict[tag]] += 1\n",
    "            \n",
    "            prev_tag = tag\n",
    "    \n",
    "    new_feature_tags = { 'PAGE_SEP' : 'xAMITx', 'URLS' : 'xAMITx', 'NUMERICS' : 'xAMITx' }\n",
    "    try:\n",
    "        for feature in feature_tags:\n",
    "            possible_tags = feature_tags[feature]\n",
    "            possible_tags_counter = Counter(possible_tags)\n",
    "            most_common_tags = possible_tags_counter.most_common(1)\n",
    "            if len(most_common_tags) > 0:\n",
    "                best_possible_tag, tag_count = most_common_tags[0]\n",
    "            \n",
    "                if tag_count > feature_counts[feature] * 0.35:\n",
    "                    new_feature_tags[feature] = best_possible_tag\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    transition_matrix = transition_matrix + incrementer\n",
    "    \n",
    "    probability_transition_matrix = transition_matrix/transition_matrix.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # print \"Transition Values aree NaN : \", np.argwhere(np.isnan(probability_transition_matrix))\n",
    "#     probability_transition_matrix[np.isnan(probability_transition_matrix)] = incrementer\n",
    "#     probability_transition_matrix = np.log(probability_transition_matrix)\n",
    "    return probability_transition_matrix.tolist(), tags_index_dict, tags_index_dict_reverse, new_feature_tags\n",
    "        \n",
    "\n",
    "def getUniqueWords(tagged_data):\n",
    "    words = []\n",
    "    for line in tagged_data:\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        \n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            words.append(word)\n",
    "    return list(set(words))\n",
    "\n",
    "\n",
    "def computeEmissionProbabilities(tagged_data, tags_dict):\n",
    "    global incrementer\n",
    "    tags = tags_dict.keys()\n",
    "    tags.sort()\n",
    "    \n",
    "    words = getUniqueWords(tagged_data)\n",
    "    words.sort()\n",
    "    \n",
    "    tags_index_dict = {}\n",
    "    for index, tag in enumerate(tags):\n",
    "        tags_index_dict[tag] = index\n",
    "        \n",
    "    words_index_dict = {}\n",
    "    words_index_dict_reverse = {}\n",
    "    for index, word in enumerate(words):\n",
    "        words_index_dict[word] = index\n",
    "        words_index_dict_reverse[index] = word\n",
    "    \n",
    "    tag_count = len(tags)\n",
    "    word_count = len(words)\n",
    "    \n",
    "    # word_count + 1 => Last column for unseen words\n",
    "    emission_matrix = np.zeros(shape=(tag_count, word_count + 1))\n",
    "    \n",
    "    for line in tagged_data:\n",
    "        prev_tag = None\n",
    "        word_tag_pairs = line.strip().split(' ')\n",
    "        \n",
    "        for word_tag_pair in word_tag_pairs:\n",
    "            word, tag = splitWordTag(word_tag_pair)\n",
    "            \n",
    "            emission_matrix[tags_index_dict[tag]][words_index_dict[word]] += 1\n",
    "            \n",
    "            prev_tag = tag\n",
    "    #increment 1 in all the elements so that the last col for unseen words have non zero values\n",
    "#     emission_matrix = emission_matrix + incrementer\n",
    "    probability_emission_matrix = emission_matrix/emission_matrix.sum(axis=1, keepdims=True)\n",
    "    # print \"Emission Values are NaN : \", np.argwhere(np.isnan(probability_emission_matrix))\n",
    "    probability_emission_matrix[np.isnan(probability_emission_matrix)] = incrementer\n",
    "#     probability_emission_matrix = np.log(probability_emission_matrix)\n",
    "    return probability_emission_matrix.tolist(), tags_index_dict, words_index_dict, words_index_dict_reverse\n",
    "\n",
    "\n",
    "def printEmissionProbabilities(count):\n",
    "    counter = 0\n",
    "    global probability_emission_matrix, tags_index_dict, words_index_dict\n",
    "    word_count = len(words_index_dict.keys())\n",
    "    tag_count = len(tags_index_dict.keys())\n",
    "    for word, word_index in words_index_dict.iteritems():\n",
    "        for tag, tag_index in tags_index_dict.iteritems():\n",
    "            if probability_emission_matrix[tag_index][word_index] != 0:\n",
    "                print tag, \" => \", word, ' => ', probability_emission_matrix[tag_index][word_index]\n",
    "                counter += 1\n",
    "                if counter > count:\n",
    "                    return\n",
    "\n",
    "\n",
    "def writeModelToFile(probability_transition_matrix, opening_probabilities, closing_probabilities, probability_emission_matrix, tags_index_dict, words_index_dict, new_feature_tags):\n",
    "    total_tags = len(tags_index_dict.keys())\n",
    "    total_words = len(words_index_dict.keys())\n",
    "        \n",
    "    lineCounter = 7\n",
    "    text = ''\n",
    "    \n",
    "    text += '---------------------TransitionMatrix---------------------' + '\\n'\n",
    "    lineCounter += 1\n",
    "    tr_start_line_number = lineCounter\n",
    "    tr_end_line_number = tr_start_line_number\n",
    "    for row in range(len(probability_transition_matrix)):\n",
    "        row_text = ''\n",
    "        for col in range(len(probability_transition_matrix[0])):\n",
    "            row_text += str(probability_transition_matrix[row][col]) + '\\t'\n",
    "        row_text = row_text.strip()\n",
    "        text += row_text + '\\n'\n",
    "        tr_end_line_number += 1\n",
    "    \n",
    "    text += '---------------------EmissionMatrix---------------------' + '\\n'\n",
    "    \n",
    "    em_start_line_number = tr_end_line_number + 1\n",
    "    em_end_line_number = em_start_line_number\n",
    "    for row in range(len(probability_emission_matrix)):\n",
    "        row_text = ''\n",
    "        for col in range(len(probability_emission_matrix[0])):\n",
    "            row_text += str(probability_emission_matrix[row][col]) + '\\t'\n",
    "        row_text = row_text.strip()\n",
    "        text += row_text + '\\n'\n",
    "        em_end_line_number += 1\n",
    "        \n",
    "    text += '---------------------OpeningClosingProbabilities---------------------' + '\\n'\n",
    "    \n",
    "    oc_start_line_number = em_end_line_number + 1\n",
    "    oc_end_line_number = oc_start_line_number\n",
    "    for tag in opening_probabilities:\n",
    "        tag_details = tag + '\\t' + str(opening_probabilities[tag]) + '\\t' + str(closing_probabilities[tag]) + '\\t' + str(tags_index_dict[tag]) + '\\n'\n",
    "        text += tag_details\n",
    "        oc_end_line_number += 1\n",
    "    \n",
    "    text += '---------------------Words---------------------' + '\\n'\n",
    "    \n",
    "    wi_start_line_number = oc_end_line_number + 1\n",
    "    wi_end_line_number = wi_start_line_number\n",
    "        \n",
    "    for word in words_index_dict:\n",
    "        word_details = word + '\\t' + str(words_index_dict[word]) + '\\n'\n",
    "        text += word_details\n",
    "        wi_end_line_number += 1\n",
    "        \n",
    "    text += '---------------------AdditionalFeatures---------------------' + '\\n'\n",
    "        \n",
    "    af_start_line_number = wi_end_line_number + 1\n",
    "    af_end_line_number = af_start_line_number\n",
    "    \n",
    "    for feature_name in new_feature_tags.keys():\n",
    "        text += feature_name + '\\t' + new_feature_tags[feature_name] + '\\n'\n",
    "        af_end_line_number += 1\n",
    "    \n",
    "    header = ''\n",
    "    header += 'total_tags:' + str(total_tags) + '\\n'\n",
    "    header += 'total_words:' + str(total_words) + '\\n'\n",
    "    header += 'tranistion_matrix:' + str(tr_start_line_number) + ':' + str(tr_end_line_number) + '\\n'\n",
    "    header += 'emission_matrix:' + str(em_start_line_number) + ':' + str(em_end_line_number) + '\\n'\n",
    "    header += 'open_close_probabilities:' + str(oc_start_line_number) + ':' + str(oc_end_line_number) + '\\n'\n",
    "    header += 'word_indexes:' + str(wi_start_line_number) + ':' + str(wi_end_line_number) + '\\n'\n",
    "    header += 'additional_features:' + str(af_start_line_number) + ':' + str(af_end_line_number) + '\\n'\n",
    "    \n",
    "    text = header + text\n",
    "    filename = 'hmmmodel.txt'\n",
    "    with open(filename, 'w') as output_file:\n",
    "        output_file.write(text)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tagged_data = getFileFromCommandLine()\n",
    "#     tagged_data = getFileContents('data/zh_train_tagged.txt')\n",
    "    tags_dict = getUniqueTags(tagged_data)\n",
    "\n",
    "    opening_probabilities = getOpenProbabilities(tagged_data, tags_dict)\n",
    "    closing_probabilities = getCloseProbabilities(tagged_data, tags_dict)\n",
    "\n",
    "    probability_transition_matrix, tags_index_dict, tags_index_dict_reverse, new_feature_tags = buildTransitionMatrix(tagged_data, tags_dict)\n",
    "\n",
    "    probability_emission_matrix, tags_index_dict, words_index_dict, words_index_dict_reverse = computeEmissionProbabilities(tagged_data, tags_dict)\n",
    "\n",
    "    writeModelToFile(probability_transition_matrix, opening_probabilities, closing_probabilities, probability_emission_matrix, tags_index_dict, words_index_dict, new_feature_tags)\n",
    "    print \"Done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
